import torch, json
from datasets import Dataset
from transformers import (
    AutoTokenizer, AutoModelForCausalLM,
    BitsAndBytesConfig, TrainingArguments,
    DataCollatorForLanguageModeling, Trainer
)
from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model

# ==== é…ç½® ====
# model_name = "Qwen2-7B-Instruct"
# model_name = "Qwen2.5-Coder-7B-Instruct"
# model_name = "Qwen2.5-Coder-3B-Instruct"
model_name = "Qwen3-4B"
model_path = f"../../models/{model_name}"
output_dir = f"../../loraResult/{model_name}"
data_path = "../../trainData/merged_2025_07_15_19_11.jsonl"

# ==== å·¥å…·å‡½æ•° ====
def load_tokenizer(path):
    tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True)
    tokenizer.pad_token = tokenizer.pad_token or tokenizer.eos_token
    tokenizer.model_max_length = 32768  # é¿å…è¶…é•¿æ–‡æœ¬ OOM
    return tokenizer

def format_chatml(example):
    text = []
    for msg in example["messages"]:
        text.append(f"<|im_start|>{msg['role']}\n{msg['content']}<|im_end|>")
    return {"text": "\n".join(text)}

def load_dataset(path, tokenizer):
    with open(path, "r", encoding="utf-8") as f:
        raw_data = [json.loads(line) for line in f if line.strip()]
    dataset = Dataset.from_list(raw_data)
    dataset = dataset.map(format_chatml)
    dataset = dataset.map(lambda ex: tokenizer(ex["text"], truncation=True, padding="max_length", max_length=512), batched=True)
    dataset = dataset.remove_columns(["messages", "text"])
    dataset.set_format("torch")
    return dataset

def prepare_model(path):
    config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        llm_int8_enable_fp32_cpu_offload=False,  # å°½é‡å…³é—­ï¼Œé™¤éä½ æ˜¾å­˜ä¸å¤Ÿ
    )
    model = AutoModelForCausalLM.from_pretrained(path, device_map="auto", trust_remote_code=True, quantization_config=config)
    model = prepare_model_for_kbit_training(model)
    lora_cfg = LoraConfig(
        task_type="CAUSAL_LM",
        r=8,                        # LoRA çš„ç§©ï¼Œä¸€èˆ¬ 8 æˆ– 16
        lora_alpha=16,              # é€šå¸¸ä¸º r çš„ 2 å€
        lora_dropout=0.05,
        bias="none",
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]  # é€‚é… Qwen æ¶æ„
    )
    return get_peft_model(model, lora_cfg)

def print_trainable_parameters(model):
    total = sum(p.numel() for p in model.parameters())
    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"ğŸ”§ Trainable params: {trainable} / {total} ({trainable / total:.2%})")

# ==== ä¸»æµç¨‹ ====
tokenizer = load_tokenizer(model_path)
dataset = load_dataset(data_path, tokenizer)
model = prepare_model(model_path)
print_trainable_parameters(model)

args = TrainingArguments(
    output_dir=output_dir,
    num_train_epochs=3,                        # å»ºè®®è®­ç»ƒå¤šè½®ï¼Œæå‡å­¦ä¹ æ•ˆæœ
    per_device_train_batch_size=1,             # 4070S æ˜¾å­˜å¯æ”¯æ’‘ batch size 2~6ï¼Œå»ºè®®ä»4èµ·è¯•éªŒ
    gradient_accumulation_steps=4,             # ç´¯ç§¯æ¢¯åº¦æ‰©å¤§æœ‰æ•ˆ batch sizeï¼ˆå¦‚æ€» batch = 4x4 = 16ï¼‰
    learning_rate=1e-4,                        # 3e-4 å¯¹å¤§æ¨¡å‹åé«˜ï¼Œå»ºè®®å°è¯• 2e-4 æ›´ç¨³
    lr_scheduler_type="cosine",                # å­¦ä¹ ç‡è°ƒåº¦ï¼šcosine æ”¶æ•›æ›´å¹³æ»‘
    warmup_ratio=0.03,                         # ç”¨ warmup_ratio æ›¿ä»£ warmup_stepsï¼Œé€‚é…ä¸åŒæ­¥æ•°
    logging_steps=10,
    save_strategy="epoch",
    max_grad_norm=1.0,
    bf16=False,                                # 4070 ä¸æ”¯æŒ BF16
    fp16=True,                                 # å¼€å¯ FP16 æ›´çœæ˜¾å­˜ï¼ˆæ¨èï¼‰
    report_to="none"
)


trainer = Trainer(
    model=model,
    args=args,
    train_dataset=dataset,
    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)
)

trainer.train()
model.save_pretrained(output_dir)
print("LoRA å¾®è°ƒå®Œæˆï¼Œæ¨¡å‹å·²ä¿å­˜åˆ°:", output_dir)
